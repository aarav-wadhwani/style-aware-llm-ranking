<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Style-Aware LLM Ranking | Technical Report</title>
  <meta name="description" content="A rigorous statistical framework for ranking large language models using the Bradley–Terry model with covariate control for stylistic biases.">
  
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Merriweather:ital,wght@0,300;0,400;0,700;1,300&display=swap" rel="stylesheet">

  <style>
    :root {
      /* Color Palette - Academic Dark Mode */
      --bg-body: #0b0e14;
      --bg-sidebar: #111620;
      --bg-card: #161b28;
      --bg-card-hover: #1c2333;
      
      --border-subtle: rgba(255, 255, 255, 0.08);
      --border-focus: rgba(255, 255, 255, 0.15);
      
      --text-main: #e2e8f0;
      --text-muted: #94a3b8;
      --text-faint: #64748b;
      
      --accent-primary: #38bdf8; /* Sky Blue */
      --accent-secondary: #818cf8; /* Indigo */
      --accent-success: #34d399; /* Emerald */
      --accent-warning: #fbbf24; /* Amber */
      
      --font-ui: 'Inter', sans-serif;
      --font-body: 'Merriweather', serif;
      --font-mono: 'JetBrains Mono', monospace;
      
      --max-width: 1400px;
      --sidebar-width: 280px;
    }

    /* Reset & Base */
    * { box-sizing: border-box; }
    
    html { scroll-behavior: smooth; }
    
    body {
      margin: 0;
      padding: 0;
      background-color: var(--bg-body);
      color: var(--text-main);
      font-family: var(--font-body);
      line-height: 1.8;
      font-size: 17px;
    }

    a { color: var(--accent-primary); text-decoration: none; transition: 0.2s; }
    a:hover { color: var(--accent-secondary); }

    /* Layout Grid */
    .app-container {
      display: grid;
      grid-template-columns: var(--sidebar-width) 1fr;
      min-height: 100vh;
    }

    /* Sidebar Navigation */
    .sidebar {
      background: var(--bg-sidebar);
      border-right: 1px solid var(--border-subtle);
      padding: 2rem;
      position: sticky;
      top: 0;
      height: 100vh;
      overflow-y: auto;
      display: flex;
      flex-direction: column;
    }

    .brand {
      font-family: var(--font-ui);
      font-weight: 700;
      font-size: 1.2rem;
      color: var(--text-main);
      margin-bottom: 2rem;
      letter-spacing: -0.02em;
    }

    .nav-links {
      list-style: none;
      padding: 0;
      margin: 0;
      display: flex;
      flex-direction: column;
      gap: 0.5rem;
    }

    .nav-links a {
      display: block;
      padding: 0.6rem 0.8rem;
      border-radius: 6px;
      font-family: var(--font-ui);
      font-size: 0.9rem;
      color: var(--text-muted);
      border-left: 2px solid transparent;
    }

    .nav-links a:hover, .nav-links a.active {
      background: rgba(255,255,255,0.03);
      color: var(--text-main);
      border-left-color: var(--accent-primary);
    }

    .sidebar-footer {
      margin-top: auto;
      padding-top: 2rem;
      font-family: var(--font-ui);
      font-size: 0.75rem;
      color: var(--text-faint);
    }

    /* Main Content */
    main {
      padding: 3rem 4rem;
      max-width: 1200px;
      margin: 0 auto;
      width: 100%;
    }

    /* Typography */
    h1, h2, h3, h4 {
      font-family: var(--font-ui);
      color: var(--text-main);
      margin-top: 2.5rem;
      margin-bottom: 1rem;
      line-height: 1.3;
    }

    h1 {
      font-size: 2.75rem;
      font-weight: 800;
      letter-spacing: -0.03em;
      margin-top: 0;
      background: linear-gradient(120deg, var(--text-main), var(--text-muted));
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
    }

    h2 {
      font-size: 1.8rem;
      border-bottom: 1px solid var(--border-subtle);
      padding-bottom: 0.5rem;
      margin-top: 4rem;
    }

    h3 { font-size: 1.4rem; font-weight: 600; color: var(--accent-primary); }
    h4 { font-size: 1.1rem; font-weight: 600; text-transform: uppercase; letter-spacing: 0.05em; color: var(--text-muted); margin-top: 2rem;}

    p { margin-bottom: 1.5rem; color: var(--text-muted); }
    strong { color: var(--text-main); font-weight: 600; }

    /* Components */
    .abstract {
      font-size: 1.1rem;
      color: var(--text-main);
      background: linear-gradient(180deg, rgba(56, 189, 248, 0.1) 0%, rgba(56, 189, 248, 0.02) 100%);
      border: 1px solid rgba(56, 189, 248, 0.2);
      padding: 2rem;
      border-radius: 12px;
      margin-bottom: 3rem;
    }

    .tags { display: flex; gap: 0.5rem; margin-top: 1rem; flex-wrap: wrap;}
    .tag {
      font-family: var(--font-mono);
      font-size: 0.75rem;
      padding: 4px 10px;
      border-radius: 99px;
      background: rgba(255,255,255,0.05);
      border: 1px solid var(--border-subtle);
      color: var(--text-muted);
    }

    /* Cards & Figures */
    .grid-2 {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
      gap: 2rem;
      margin: 2rem 0;
    }

    .figure-card {
      background: var(--bg-card);
      border: 1px solid var(--border-subtle);
      border-radius: 12px;
      overflow: hidden;
      transition: transform 0.2s, border-color 0.2s;
    }

    .figure-card:hover {
      border-color: var(--border-focus);
      transform: translateY(-2px);
    }

    .figure-card img {
      width: 100%;
      height: auto;
      display: block;
      background: #0f111a; /* Match plot bg if possible */
    }

    figcaption {
      padding: 1rem 1.5rem;
      font-family: var(--font-ui);
      font-size: 0.85rem;
      color: var(--text-muted);
      border-top: 1px solid var(--border-subtle);
      background: rgba(0,0,0,0.2);
    }

    figcaption strong { color: var(--text-main); display: block; margin-bottom: 0.25rem; }

    /* Mathematical Notation */
    .math-block {
      font-family: 'Times New Roman', Times, serif;
      font-size: 1.2rem;
      background: rgba(0,0,0,0.3);
      padding: 1.5rem;
      border-radius: 8px;
      text-align: center;
      margin: 2rem 0;
      border: 1px solid var(--border-subtle);
      color: var(--text-main);
    }

    .math-inline {
      font-family: 'Times New Roman', Times, serif;
      font-style: italic;
      color: var(--text-main);
      padding: 0 2px;
    }

    /* Insights Box */
    .insight-box {
      border-left: 3px solid var(--accent-secondary);
      background: linear-gradient(90deg, rgba(129, 140, 248, 0.05), transparent);
      padding: 1.5rem;
      margin: 2rem 0;
      border-radius: 0 8px 8px 0;
    }
    
    .insight-title {
      font-family: var(--font-ui);
      font-weight: 700;
      color: var(--accent-secondary);
      font-size: 0.9rem;
      text-transform: uppercase;
      margin-bottom: 0.5rem;
      display: block;
    }

    /* Lists */
    ul { padding-left: 1.5rem; color: var(--text-muted); }
    li { margin-bottom: 0.5rem; }
    
    /* Code */
    code {
      font-family: var(--font-mono);
      background: rgba(255,255,255,0.08);
      padding: 2px 6px;
      border-radius: 4px;
      font-size: 0.85em;
      color: var(--text-main);
    }

    /* Responsive */
    @media (max-width: 900px) {
      .app-container { grid-template-columns: 1fr; }
      .sidebar { 
        height: auto; 
        position: relative; 
        border-right: none; 
        border-bottom: 1px solid var(--border-subtle);
        padding: 1.5rem;
      }
      main { padding: 2rem 1.5rem; }
      .grid-2 { grid-template-columns: 1fr; }
      h1 { font-size: 2rem; }
    }
  </style>
</head>
<body>

<div class="app-container">
  <aside class="sidebar">
    <div class="brand">LLM Rankings</div>
    <nav>
      <ul class="nav-links">
        <li><a href="#executive-summary">Executive Summary</a></li>
        <li><a href="#data-landscape">Data Landscape</a></li>
        <li><a href="#methodology">Statistical Methodology</a></li>
        <li><a href="#baseline-results">Baseline Rankings</a></li>
        <li><a href="#length-bias">The Length Bias Problem</a></li>
        <li><a href="#style-control">Style-Controlled Results</a></li>
        <li><a href="#domain-analysis">Domain Specificity</a></li>
        <li><a href="#conclusion">Conclusion</a></li>
      </ul>
    </nav>
    <div class="sidebar-footer">
      <p>Deployed via GitHub Pages<br>Last Updated: Dec 2025</p>
    </div>
  </aside>

  <main>
    <section id="executive-summary">
      <h1>Style-Aware LLM Ranking</h1>
      <div class="tags">
        <span class="tag">Bradley-Terry Model</span>
        <span class="tag">Bootstrap Inference</span>
        <span class="tag">Covariate Control</span>
        <span class="tag">Statistical Learning</span>
      </div>
      
      <p style="font-size: 1.2rem; color: var(--text-main); margin-top: 1.5rem;">
        Large Language Model (LLM) evaluation often relies on aggregated win-rates from pairwise human preferences. However, this approach is susceptible to imbalances in evaluation data and superficial stylistic confounds—most notably, response length.
      </p>

      <div class="abstract">
        <strong>Abstract:</strong> This report presents a rigorous statistical framework for ranking LLMs. By utilizing the <strong>Bradley–Terry model</strong> with style covariates, we decouple intrinsic content quality from presentation hacks. We analyze a large-scale dataset of pairwise battles, identifying that simple verbosity is a strong predictor of human preference. Our style-controlled ranking reveals "true" model capability, penalizing models that rely on excessive length to win battles.
      </div>
    </section>

    <hr style="border: 0; border-top: 1px solid var(--border-subtle); margin: 3rem 0;">

    <section id="data-landscape">
      <h2>1. The Data Landscape</h2>
      <p>
        The foundation of this analysis is a dataset of crowdsourced pairwise comparisons ("battles"). Before modeling, it is crucial to inspect the distributional properties of the data to identify potential biases such as class imbalance or sparse coverage.
      </p>

      <div class="grid-2">
        <figure class="figure-card">
          <img src="results/figures/newplot%20(11).png" alt="Distribution of battle outcomes">
          <figcaption>
            <strong>Figure 1: Battle Outcome Distribution</strong>
            The dataset includes wins for Model A, wins for Model B, and ties. Ties are excluded from the primary Bradley-Terry estimation to maximize signal clarity.
          </figcaption>
        </figure>
        <figure class="figure-card">
          <img src="results/figures/newplot%20(12).png" alt="Battle count per model">
          <figcaption>
            <strong>Figure 2: Model Exposure Balance</strong>
            Significant variance exists in battle counts. Popular models have thousands of evaluations, while newer ones have fewer. This motivates the use of probabilistic ranking over simple win-rates.
          </figcaption>
        </figure>
      </div>

      <div class="insight-box">
        <span class="insight-title">Data Insight: Sparsity & Transitivity</span>
        <p style="margin:0">
          Naive win-rate calculations fail when the comparison matrix is sparse (i.e., not every model has fought every other model). The heatmaps below (Fig 3 & 4) show the density of comparisons. The Bradley-Terry model solves this by leveraging <strong>transitivity</strong>: if $A > B$ and $B > C$, we can infer $A > C$ even without direct battles.
        </p>
      </div>

      <div class="grid-2">
        <figure class="figure-card">
          <img src="results/figures/newplot%20(13).png" alt="Heatmap of all battles">
          <figcaption><strong>Figure 3:</strong> Total pairwise battle matrix (All outcomes).</figcaption>
        </figure>
        <figure class="figure-card">
          <img src="results/figures/newplot%20(14).png" alt="Heatmap of battles without ties">
          <figcaption><strong>Figure 4:</strong> Pairwise battle matrix (Ties removed).</figcaption>
        </figure>
      </div>

      <h3>Demographics & Interaction Complexity</h3>
      <p>
        The dataset is heavily English-dominated but includes a long tail of multilingual interactions. Most interactions are short (1-2 turns), suggesting the ranking primarily reflects "zero-shot" or "few-shot" QA capability rather than long-context maintenance.
      </p>
      
      <div class="grid-2">
        <figure class="figure-card">
          <img src="results/figures/newplot%20(15).png" alt="Language distribution">
          <figcaption><strong>Figure 5:</strong> Language distribution showing English dominance.</figcaption>
        </figure>
        <figure class="figure-card">
          <img src="results/figures/newplot%20(16).png" alt="Conversation turns">
          <figcaption><strong>Figure 6:</strong> Distribution of conversation turns per battle.</figcaption>
        </figure>
      </div>
    </section>

    <section id="methodology">
      <h2>2. Statistical Methodology</h2>
      <p>
        To move beyond simple win rates, we employ the <strong>Bradley–Terry (BT) model</strong>, a standard in psychometrics for pairwise comparisons. This probabilistic model assumes that the probability of model $i$ beating model $j$ depends on the difference in their latent strength parameters, $\theta$.
      </p>

      <h3>The Standard Model</h3>
      <div class="math-block">
        P(i \succ j) = \frac{1}{1 + e^{-(\theta_i - \theta_j)}}
      </div>
      <p>
        Where $\theta_i$ is the intrinsic skill of model $i$. We estimate these parameters via Maximum Likelihood Estimation (MLE) by minimizing the negative log-likelihood of the observed battle outcomes.
      </p>

      <h3>The Style-Adjusted Model</h3>
      <p>
        Human annotators are imperfect. They may be swayed by superficial formatting (bold text, headers) or verbosity. To isolate "true" quality, we introduce a vector of style covariates $S_{ij}$ (e.g., difference in length, difference in markdown usage).
      </p>
      <div class="math-block">
         P(i \succ j) = \sigma(\theta_i - \theta_j + \beta^T \cdot S_{ij})
      </div>
      <p>
        Here, $\beta$ represents the coefficients for stylistic features. If $\beta_{length}$ is positive and significant, it proves that users prefer longer answers <em>regardless of the model's identity</em>. The adjusted ranking is derived from $\theta$ after controlling for these $\beta$ terms.
      </p>

      <h3>Uncertainty Quantification</h3>
      <p>
        We use <strong>Bootstrap Resampling</strong> ($N=1000$) to generate 95% Confidence Intervals (CIs). This ensures we do not over-interpret small differences in rank between models with overlapping performance bands.
      </p>
      <figure class="figure-card" style="max-width: 800px; margin: 0 auto;">
        <img src="results/figures/newplot%20(26).png" alt="Bradley-Terry Scores with CI">
        <figcaption>
          <strong>Figure 7: Model Strength Estimates ($\theta$) with 95% CIs.</strong>
          The error bars represent the stability of the ranking. Overlapping bars indicate statistical ties.
        </figcaption>
      </figure>
    </section>

    <section id="baseline-results">
      <h2>3. Baseline Rankings (Uncontrolled)</h2>
      <p>
        Before applying corrections, we examine the standard "Leaderboard" view. This represents the raw preference of the human crowd, incorporating all biases.
      </p>

      <div class="grid-2">
        <figure class="figure-card">
          <img src="results/figures/newplot%20(27).png" alt="Overall Rank Heatmap">
          <figcaption><strong>Figure 8:</strong> Overall Model Ranking (Heatmap View).</figcaption>
        </figure>
        <figure class="figure-card">
          <img src="results/figures/newplot%20(17).png" alt="Pairwise Win Rate Matrix">
          <figcaption><strong>Figure 9:</strong> Empirical Pairwise Win-Rates. Darker squares indicate higher dominance.</figcaption>
        </figure>
      </div>

      <h4>Robustness Check</h4>
      <p>Are these rankings driven only by the most popular prompts? We re-ran the analysis excluding the top prompts to check for "overfitting" to common queries.</p>
      <div class="grid-2">
        <figure class="figure-card">
          <img src="results/figures/newplot%20(18).png" alt="Average Win Rate">
          <figcaption><strong>Figure 10:</strong> Average Win Rate (Global).</figcaption>
        </figure>
        <figure class="figure-card">
          <img src="results/figures/newplot%20(19).png" alt="Win Rate excluding top prompts">
          <figcaption><strong>Figure 11:</strong> Win Rate excluding top frequent prompts. The high correlation suggests the rankings are robust to prompt distribution.</figcaption>
        </figure>
      </div>
    </section>

    <section id="length-bias">
      <h2>4. The Length Bias Problem</h2>
      <p>
        A critical finding of this report is the strong correlation between <strong>verbosity</strong> and <strong>rank</strong>. Evaluators systematically prefer longer responses, even when the content quality might be equivalent.
      </p>

      <figure class="figure-card">
        <img src="results/figures/newplot%20(28).png" alt="Response length vs Rank">
        <figcaption>
          <strong>Figure 12: Verbosity vs. Rank Correlation.</strong>
          Models are ordered by their baseline rank (x-axis). The y-axis shows average response length in tokens. The trend line is undeniably positive: higher-ranked models tend to be significantly more verbose.
        </figcaption>
      </figure>

      <div class="insight-box" style="border-left-color: var(--accent-warning);">
        <span class="insight-title" style="color: var(--accent-warning);">Bias Alert</span>
        <p>
          This correlation poses a risk to the ecosystem: it encourages "bloat." If developers optimize purely for leaderboard rank, they may fine-tune models to output filler text rather than concise, helpful answers.
        </p>
      </div>
    </section>

    <section id="style-control">
      <h2>5. Style-Controlled Results</h2>
      <p>
        By applying the style-adjusted Bradley-Terry model, we can estimate the impact of formatting features ($\beta$) and derive a "purified" ranking ($\theta$).
      </p>

      <h3>Feature Importance</h3>
      <p>Which superficial features matter most? We analyzed Length, Bold tags, LaTeX usage, and List formatting.</p>
      
      <figure class="figure-card">
        <img src="results/figures/newplot%20(30).png" alt="Style coefficients">
        <figcaption>
          <strong>Figure 13: Style Coefficients ($\beta$).</strong>
          The coefficient for "Length" is the largest and most positive. This confirms that, holding content constant, simply writing <em>more</em> increases the probability of winning a battle.
        </figcaption>
      </figure>

      <h3>The "True" Ranking</h3>
      <p>
        Figure 14 compares the Baseline Rank vs. the Style-Controlled Rank. Shifts in this chart are revealing:
      </p>
      <ul>
        <li><strong>Downward Shift:</strong> Models that lose rank (move down) when style is controlled were likely "gaming" the system with verbosity or formatting.</li>
        <li><strong>Upward Shift:</strong> Models that gain rank are concise, high-quality models that were previously unfairly penalized for being brief.</li>
      </ul>

      <figure class="figure-card">
        <img src="results/figures/newplot%20(29).png" alt="Rank Shift Comparison">
        <figcaption><strong>Figure 14:</strong> The "Shakedown." Comparing Baseline vs. Style-Controlled rankings.</figcaption>
      </figure>
    </section>

    <section id="domain-analysis">
      <h2>6. Domain Specificity</h2>
      <p>
        General capability is a useful metric, but practical applications are often domain-specific. We clustered prompts into semantic categories (Coding, Math, Creative Writing, etc.) using embedding vectors (SVD visualization below).
      </p>

      <div class="grid-2">
        <figure class="figure-card">
          <img src="results/figures/newplot%20(23).png" alt="Clustering Runtime">
          <figcaption><strong>Figure 15:</strong> Clustering Runtime Analysis.</figcaption>
        </figure>
        <figure class="figure-card">
          <img src="results/figures/newplot%20(24).png" alt="Clustering Elbow Plot">
          <figcaption><strong>Figure 16:</strong> Elbow Plot to determine optimal $K$ categories.</figcaption>
        </figure>
      </div>

      <div class="grid-2">
        <figure class="figure-card">
          <img src="results/figures/newplot%20(25).png" alt="SVD of Prompts">
          <figcaption><strong>Figure 17:</strong> Semantic Projection (SVD) of the prompt space. Distinct clusters emerge for coding (dense) vs. creative writing (diffuse).</figcaption>
        </figure>
        <figure class="figure-card">
          <img src="results/figures/newplot%20(20).png" alt="Battles per category">
          <figcaption><strong>Figure 18:</strong> Distribution of battles across identified categories.</figcaption>
        </figure>
      </div>

      <h3>Category-Specific Leaderboards</h3>
      <p>
        Finally, we fit separate Bradley-Terry models for each category. Figure 19 reveals the specialists. Note how some models dominate "Code" but struggle in "Creative Writing."
      </p>
      
      <figure class="figure-card">
        <img src="results/figures/newplot%20(22).png" alt="Category Heatmap">
        <figcaption>
          <strong>Figure 19: Category-Specific Rank Heatmap.</strong>
          Darker blue indicates a higher rank (Model 1 is best). This visualization allows users to select the best model for their specific use case.
        </figcaption>
      </figure>
    </section>

    <section id="conclusion">
      <h2>7. Conclusion</h2>
      <p>
        This report demonstrates that while crowdsourced evaluation is a powerful signal, it is noisy and biased. A raw win-rate leaderboard incentivizes verbosity and overlooks the nuances of domain specialization.
      </p>
      <p>
        <strong>Key Takeaways:</strong>
      </p>
      <ul>
        <li><strong>Statistical Rigor:</strong> The Bradley-Terry model with bootstrap intervals provides a necessary safety margin against over-interpreting close ranks.</li>
        <li><strong>The Verbosity Trap:</strong> Length bias is real and significant ($p < 0.05$). Future evaluations must control for this to encourage efficiency.</li>
        <li><strong>No One Size Fits All:</strong> The category heatmaps show that "General Purpose" models often lag behind specialists in Code and Math.</li>
      </ul>
    </section>
  </main>
</div>

</body>
</html>